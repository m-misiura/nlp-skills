---
title: "An introduction to text generation"
author: "Mac Misiura"
date: today
format:
  revealjs:
    self-contained: true
    from: markdown+emoji
    slide-number: true
    execute:
      echo: true
---

# Diving Deeper: Understanding Text Generation Models

## Model Architecture: Behind the Scenes of GPT-2

Let's explore how transformer-based models like GPT-2 actually work to generate text:

```{python}
#| output-location: slide
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Load a small GPT-2 model
model_name = "gpt2"  # We use the smallest GPT-2 variant (124M parameters)
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# 1. Understanding the tokenization process
text = "The cat sat on the"
input_ids = tokenizer(text, return_tensors="pt").input_ids
print(f"Input text: '{text}'")
print(f"Tokenized into: {input_ids}")
print(f"Decoded tokens: {[tokenizer.decode([id]) for id in input_ids[0]]}")
```

## Token Embeddings: From Tokens to Vectors

Each token is converted into a high-dimensional vector. Let's visualize this process:

```{python}
#| output-location: slide
# Get the embedding layer from the model
embeddings = model.transformer.wte(input_ids)
print(f"Embedding shape: {embeddings.shape}") 
# This shows [batch_size, sequence_length, embedding_dim]

# Visualize the first token's embedding
plt.figure(figsize=(10, 4))
sns.heatmap(embeddings[0, 0, :50].detach().reshape(5, 10), annot=True, cmap="viridis")
plt.title("First 50 dimensions of embedding for token: " + tokenizer.decode(input_ids[0][0]))
plt.xlabel("Embedding Dimension")
plt.ylabel("Embedding Dimension")
plt.show()
```

## Attention Mechanism: How Models Focus

The heart of transformer models is the attention mechanism. Let's see it in action:

```{python}
#| output-location: slide
# Get attention values
with torch.no_grad():
    outputs = model(input_ids, output_attentions=True)
    attentions = outputs.attentions

# Visualize attention in the first layer, first head
attention_matrix = attentions[0][0, 0].detach().numpy()
tokens = [tokenizer.decode([id]) for id in input_ids[0]]

plt.figure(figsize=(10, 8))
sns.heatmap(attention_matrix, xticklabels=tokens, yticklabels=tokens, cmap="viridis", annot=True)
plt.title("Attention Map (Layer 1, Head 1)")
plt.xlabel("Key Tokens")
plt.ylabel("Query Tokens")
plt.show()
```

## Token Prediction: Generating the Next Word

Now let's see how the model predicts the next token:

```{python}
#| output-location: slide
# Get logits (raw prediction scores) for the next token
with torch.no_grad():
    outputs = model(input_ids)
    next_token_logits = outputs.logits[:, -1, :]

# Convert logits to probabilities
next_token_probs = torch.softmax(next_token_logits, dim=-1)

# Get the top 5 predictions
topk = 5
top_k_probs, top_k_indices = torch.topk(next_token_probs, topk, dim=-1)

# Display the top 5 predicted tokens and their probabilities
print("\nTop 5 predicted next tokens:")
for i in range(topk):
    token = tokenizer.decode([top_k_indices[0, i]])
    prob = top_k_probs[0, i].item() * 100
    print(f"{i+1}. '{token}' with probability {prob:.2f}%")
```

## Generating Complete Sentences

Let's put it all together and see how text generation works step by step:

```{python}
#| output-location: slide
def generate_step_by_step(prompt, max_length=20):
    input_ids = tokenizer(prompt, return_tensors="pt").input_ids
    generated_text = prompt
    
    print(f"Starting with: '{prompt}'")
    
    for _ in range(max_length):
        # Get predictions
        with torch.no_grad():
            outputs = model(input_ids)
            next_token_logits = outputs.logits[:, -1, :]
        
        # Get probabilities
        next_token_probs = torch.softmax(next_token_logits, dim=-1)
        
        # Sample from the distribution
        next_token = torch.multinomial(next_token_probs, num_samples=1)
        
        # Add the predicted token to the sequence
        input_ids = torch.cat([input_ids, next_token], dim=-1)
        
        # Convert to text
        new_text = tokenizer.decode(input_ids[0])
        
        # Get just the newly added token
        new_token = tokenizer.decode([next_token[0, 0]])
        
        # Get the probability of the chosen token
        chosen_prob = next_token_probs[0, next_token[0, 0]].item() * 100
        
        print(f"Added: '{new_token}' (probability: {chosen_prob:.2f}%)")
        print(f"Text so far: '{new_text}'")
        
        # Break if we encounter the end of sequence token
        if next_token[0, 0] == tokenizer.eos_token_id:
            break
            
    return new_text

# Generate text step by step
generated = generate_step_by_step("The cat sat on the")
```

## Sampling Strategies: Controlling Text Generation

Different sampling methods can dramatically change the output:

```{python}
#| output-location: slide
def compare_sampling_methods(prompt):
    # Initialize the models and tokenizers
    tokenizer = AutoTokenizer.from_pretrained("gpt2")
    model = AutoModelForCausalLM.from_pretrained("gpt2")
    
    # Different generation methods
    methods = {
        "Greedy": {"do_sample": False},
        "Temperature (0.7)": {"do_sample": True, "temperature": 0.7},
        "Top-k (50)": {"do_sample": True, "top_k": 50},
        "Top-p (0.9)": {"do_sample": True, "top_p": 0.9},
        "Top-k (50) + Top-p (0.9)": {"do_sample": True, "top_k": 50, "top_p": 0.9}
    }
    
    print(f"Prompt: '{prompt}'")
    print("-" * 50)
    
    # Run each method
    for name, params in methods.items():
        input_ids = tokenizer(prompt, return_tensors="pt").input_ids
        
        # Set a random seed for reproducibility in sampling methods
        torch.manual_seed(42)
        
        # Generate
        outputs = model.generate(
            input_ids, 
            max_length=30,
            **params
        )
        
        text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(f"{name}:\n{text}\n")

# Compare different sampling methods
compare_sampling_methods("The cat sat on the")
```

## Building a Simple Transformer from Scratch

To really understand how these models work, let's build a tiny version from scratch:

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class SelfAttention(nn.Module):
    def __init__(self, embed_size, heads):
        super(SelfAttention, self).__init__()
        self.embed_size = embed_size
        self.heads = heads
        self.head_dim = embed_size // heads
        
        self.query = nn.Linear(embed_size, embed_size)
        self.key = nn.Linear(embed_size, embed_size)
        self.value = nn.Linear(embed_size, embed_size)
        self.fc_out = nn.Linear(embed_size, embed_size)
        
    def forward(self, x):
        batch_size = x.shape[0]
        seq_length = x.shape[1]
        
        # Split embedding into self.heads pieces
        q = self.query(x).reshape(batch_size, seq_length, self.heads, self.head_dim)
        k = self.key(x).reshape(batch_size, seq_length, self.heads, self.head_dim)
        v = self.value(x).reshape(batch_size, seq_length, self.heads, self.head_dim)
        
        # Transpose for attention computation
        q = q.transpose(1, 2)  # (batch_size, heads, seq_length, head_dim)
        k = k.transpose(1, 2)
        v = v.transpose(1, 2)
        
        # Calculate attention scores
        # (batch_size, heads, seq_length, head_dim) x (batch_size, heads, head_dim, seq_length)
        # -> (batch_size, heads, seq_length, seq_length)
        attention = torch.matmul(q, k.transpose(-1, -2))
        attention = attention / math.sqrt(self.head_dim)
        
        # We create a lower triangular matrix of -inf to ensure autoregressive property
        # This creates the "masked" part of masked self-attention
        mask = torch.tril(torch.ones(seq_length, seq_length)).view(
            1, 1, seq_length, seq_length
        )
        attention = attention.masked_fill(mask == 0, float("-inf"))
        
        # Apply softmax to get attention weights summing to 1
        attention = F.softmax(attention, dim=-1)
        
        # Apply attention to values
        # (batch_size, heads, seq_length, seq_length) x (batch_size, heads, seq_length, head_dim)
        # -> (batch_size, heads, seq_length, head_dim)
        out = torch.matmul(attention, v)
        
        # Reshape back to original size
        out = out.transpose(1, 2).contiguous().view(batch_size, seq_length, self.embed_size)
        
        return self.fc_out(out)

class TinyTransformerBlock(nn.Module):
    def __init__(self, embed_size, heads, dropout, forward_expansion):
        super(TinyTransformerBlock, self).__init__()
        self.attention = SelfAttention(embed_size, heads)
        self.norm1 = nn.LayerNorm(embed_size)
        self.norm2 = nn.LayerNorm(embed_size)
        
        self.feed_forward = nn.Sequential(
            nn.Linear(embed_size, forward_expansion * embed_size),
            nn.GELU(),
            nn.Linear(forward_expansion * embed_size, embed_size),
        )
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x):
        # Attention block with residual connection
        attention = self.attention(x)
        x = self.norm1(attention + x)
        x = self.dropout(x)
        
        # Feedforward block with residual connection
        forward = self.feed_forward(x)
        x = self.norm2(forward + x)
        x = self.dropout(x)
        
        return x

class TinyGPT(nn.Module):
    def __init__(
        self,
        vocab_size,
        embed_size,
        num_layers,
        heads,
        max_length,
        forward_expansion,
        dropout,
    ):
        super(TinyGPT, self).__init__()
        self.embed_size = embed_size
        self.token_embedding = nn.Embedding(vocab_size, embed_size)
        self.position_embedding = nn.Embedding(max_length, embed_size)
        
        self.layers = nn.ModuleList(
            [
                TinyTransformerBlock(
                    embed_size, heads, dropout, forward_expansion
                )
                for _ in range(num_layers)
            ]
        )
        
        self.fc_out = nn.Linear(embed_size, vocab_size)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x):
        batch_size, seq_length = x.shape
        
        # Create positions tensor
        positions = torch.arange(0, seq_length).expand(batch_size, seq_length).to(x.device)
        
        # Get embeddings
        token_embeddings = self.token_embedding(x)
        position_embeddings = self.position_embedding(positions)
        
        # Add embeddings
        x = token_embeddings + position_embeddings
        x = self.dropout(x)
        
        # Pass through transformer blocks
        for layer in self.layers:
            x = layer(x)
            
        # Project to vocabulary
        x = self.fc_out(x)
        
        return x

# Initialize a tiny GPT model
tiny_gpt = TinyGPT(
    vocab_size=100,    # Small vocab for demo
    embed_size=64,     # Much smaller than real models
    num_layers=2,      # Real GPT-2 has 12-48 layers
    heads=4,           # Real GPT-2 has 12-96 heads
    max_length=20,     # Maximum sequence length
    forward_expansion=2, # Expansion factor for FF network
    dropout=0.1,       # Dropout probability
)

# Print the model architecture
print(tiny_gpt)
```

## Key Takeaways: How Text Generation Really Works

1. **Tokenization**: Text is split into tokens, which are then converted to token IDs.

2. **Embeddings**: Each token ID is converted into a dense vector representation.

3. **Transformer Processing**: The model applies several transformer layers:
   - Self-attention mechanisms capture relationships between tokens
   - Feed-forward networks process these relationships
   - Layer normalization and residual connections stabilize training

4. **Next Token Prediction**: The model predicts probability distributions for the next token.

5. **Token Selection**: Various sampling strategies determine which token to choose:
   - Greedy: Always pick the most likely token
   - Temperature sampling: Control randomness
   - Top-k/Top-p: Limit selection to most probable tokens

6. **Iterative Generation**: The process repeats, with each new token added to the input.
