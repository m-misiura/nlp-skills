apiVersion: v1
kind: Service
metadata:
  name: minio-llm
spec:
  ports:
    - name: minio-client-port
      port: 9000
      protocol: TCP
      targetPort: 9000
  selector:
    app: minio-llm
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: llm-models-claim
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 50Gi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-container-deployment
  labels:
    app: minio-llm
spec:
  replicas: 1
  selector:
    matchLabels:
      app: minio-llm
  template:
    metadata:
      labels:
        app: minio-llm
        maistra.io/expose-route: 'true'
      name: minio-llm
    spec:
      volumes:
        - name: model-volume
          persistentVolumeClaim:
            claimName: llm-models-claim
      initContainers:
        - name: download-model
          image: quay.io/rh-ee-mmisiura/llm-downloader-bootstrap:latest
          securityContext:
            fsGroup: 1001
          command:
            - bash
            - -c
            - |
              model="m-misiura-rh/bert-tiny-llm-router"
              echo "Starting model download from Hugging Face"
              /tmp/venv/bin/huggingface-cli download $model --local-dir /mnt/models/llms/$(basename $model)
              echo "Model downloaded, starting caikit bootstrap"
              export ALLOW_DOWNLOADS=1
              source /tmp/venv/bin/activate
              python3 <<EOF
              import os
              from caikit_nlp.modules.text_classification.sequence_classification import SequenceClassification
              model_id = "/mnt/models/llms/$(basename $model)"
              output_model_dir = "/mnt/models/llms/caikit/$(basename $model)"
              os.makedirs(output_model_dir, exist_ok=True)
              SequenceClassification.bootstrap(model_id).save(output_model_dir)
              print("Caikit bootstrap complete!")
              EOF
          resources:
            limits:
              memory: "5Gi"
              cpu: "2"
          volumeMounts:
            - mountPath: "/mnt/models/"
              name: model-volume
      containers:
        - args:
            - server
            - /models
          env:
            - name: MINIO_ACCESS_KEY
              value: THEACCESSKEY
            - name: MINIO_SECRET_KEY
              value: THESECRETKEY
          image: quay.io/trustyai/modelmesh-minio-examples:latest
          name: minio-llm
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: "/models/"
              name: model-volume
---
apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  name: caikit-text-classification-runtime
  labels:
    opendatahub.io/dashboard: 'true'
  annotations:
    opendatahub.io/apiProtocol: REST
    opendatahub.io/template-display-name: Caikit Text Classification ServingRuntime
    openshift.io/display-name: Caikit Text Classification Runtime
spec:
  annotations:
    prometheus.io/path: /metrics
    prometheus.io/port: "8086"
  containers:
    - name: kserve-container
      image: quay.io/modh/caikit-nlp@sha256:7af5cfa5617723fc9039a01189e6e5e8d8d1d507cd015442588083a386618843
      command:
        - python
        - -m
        - caikit.runtime
      env:
        - name: RUNTIME_LOCAL_MODELS_DIR
          value: /mnt/models
        - name: HF_HOME
          value: /tmp/hf_home
        - name: RUNTIME_GRPC_ENABLED
          value: "false"
        - name: RUNTIME_HTTP_ENABLED
          value: "true"
      ports:
        - containerPort: 8080
          protocol: TCP
      livenessProbe:
        exec:
          command:
            - python
            - -m
            - caikit_health_probe
            - liveness
        initialDelaySeconds: 5
      readinessProbe:
        exec:
          command:
            - python
            - -m
            - caikit_health_probe
            - readiness
        initialDelaySeconds: 5
      startupProbe:
        httpGet:
          path: /health
          port: 8080
        failureThreshold: 24
        periodSeconds: 30
      volumeMounts:
        - name: shm
          mountPath: /dev/shm
  volumes:
    - name: shm
      emptyDir:
        medium: Memory
        sizeLimit: 2Gi
  multiModel: false
  supportedModelFormats:
    - name: caikit
      autoSelect: true
---
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: bert-text-classifier
  labels:
    opendatahub.io/dashboard: 'true'
  annotations:
    openshift.io/display-name: BERT Text Classifier
    serving.kserve.io/deploymentMode: RawDeployment
spec:
  predictor:
    maxReplicas: 1
    minReplicas: 1
    model:
      modelFormat:
        name: caikit
      resources:
        limits:
          cpu: 2
          memory: 8Gi
        requests:
          cpu: 1
          memory: 4Gi
      runtime: caikit-text-classification-runtime
      storage:
        key: aws-connection-minio-data-connection
        path: caikit/
---
apiVersion: v1
kind: Secret
metadata:
  name: aws-connection-minio-data-connection
  labels:
    opendatahub.io/dashboard: 'true'
    opendatahub.io/managed: 'true'
  annotations:
    opendatahub.io/connection-type: s3
    openshift.io/display-name: Minio LLM Data Connection
data:
  AWS_ACCESS_KEY_ID: VEhFQUNDRVNTS0VZ
  AWS_DEFAULT_REGION: dXMtc291dGg=
  AWS_S3_BUCKET: bGxtcw==
  AWS_S3_ENDPOINT: aHR0cDovL21pbmlvLWxsbTo5MDAw
  AWS_SECRET_ACCESS_KEY: VEhFU0VDUkVUS0VZ
type: Opaque
---